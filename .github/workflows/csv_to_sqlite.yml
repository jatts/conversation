name: Full Workflow - Merge Excel to SQLite ZIP

on:
  push:
    paths:
      # This workflow will be triggered if either of these files are pushed
      - 'conversation/csv/scanning.xlsx'
      - 'conversation/csv/prices.xlsx'
      # Also trigger if version.txt is pushed (though Phase 1 will usually update it)
      - 'conversation/Temp/version.txt'

jobs:
  # PHASE 1: Merge scanning.xlsx + prices.xlsx to Temp SQLite
  merge_and_prepare_temp_db:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Check if both files exist
        run: |
          if [ ! -f "conversation/csv/scanning.xlsx" ] || [ ! -f "conversation/csv/prices.xlsx" ]; then
            echo "Both scanning.xlsx and prices.xlsx must exist in the repository!"
            exit 1
          fi

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Required Python Packages
        run: pip install pandas openpyxl sqlite-utils

      - name: Merge XLSX Files and Create Temp SQLite + Log Files
        run: |
          python3 <<EOF
          import pandas as pd
          import sqlite3
          import os
          from datetime import datetime
          import traceback

          try:
              scan_file = "conversation/csv/scanning.xlsx"
              price_file = "conversation/csv/prices.xlsx"

              os.makedirs("conversation/Temp", exist_ok=True)
              log_path = "conversation/Temp/conversion_log.txt"

              with open(log_path, "w") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 1: Files received. Starting merge...\n")

              scan_df = pd.read_excel(scan_file)
              price_df = pd.read_excel(price_file)

              price_df["OriginalPrice"] = pd.to_numeric(price_df["OriginalPrice"], errors='coerce')
              price_df = price_df[["Barcode", "OriginalPrice"]]

              merged_df = pd.merge(scan_df, price_df, on="Barcode", how="left")

              merged_df["OriginalPrice"] = merged_df["OriginalPrice"].apply(
                  lambda x: str(int(x)) if pd.notna(x) and x == int(x) else ('' if pd.isna(x) else str(x))
              )

              final_columns = ["Barcode", "Article", "Percentage", "OriginalPrice"]
              merged_df = merged_df[final_columns]

              db_name = "conversation/Temp/temp.db"
              conn = sqlite3.connect(db_name)
              merged_df.to_sql("sc", conn, if_exists="replace", index=False, dtype={'OriginalPrice': 'TEXT'})
              conn.commit()
              conn.execute("VACUUM;")
              conn.close()

              with open(log_path, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 1: Temp SQLite created. Proceeding to next phase.\n")

              # Generate a default version.txt here if one isn't pushed manually
              # This ensures Phase 2 always has a version.txt to work with
              version_name = os.path.splitext(os.path.basename(scan_file))[0]
              with open("conversation/Temp/version.txt", "w") as vfile:
                  vfile.write(version_name + "\n")
              
              with open(log_path, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 1: Default version.txt created: {version_name}\n")

          except Exception as e:
              os.makedirs("conversation/Temp", exist_ok=True)
              log_path_error = "conversation/Temp/conversion_log.txt"
              with open(log_path_error, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 1: Error occurred: {str(e)}\n")
                  logfile.write(traceback.format_exc())
              raise

          EOF

      - name: Upload Temp Files for Next Job
        # Changed from v3 to v4
        uses: actions/upload-artifact@v4
        with:
          name: temp-merge-artifacts
          path: |
            conversation/Temp/temp.db
            conversation/Temp/conversion_log.txt
            conversation/Temp/version.txt
          # retention-days is now part of upload-artifact v4's top-level options, 
          # but typically not needed when artifacts are immediately downloaded by a subsequent job
          # You might remove it if you don't need longer retention for debugging
          retention-days: 1 

  # PHASE 2: Finalize SQLite ZIP
  finalize_db_with_version:
    needs: merge_and_prepare_temp_db
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Download Temp Files from Previous Job
        # Changed from v3 to v4
        uses: actions/download-artifact@v4
        with:
          name: temp-merge-artifacts
          path: conversation/Temp/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Required Python Packages
        run: pip install sqlite-utils

      - name: Rename Temp DB, Zip It, Update Logs
        run: |
          python3 <<EOF
          import sqlite3
          import zipfile
          import os
          from datetime import datetime
          import traceback

          try:
              version_path = "conversation/Temp/version.txt"
              if not os.path.exists(version_path):
                  raise FileNotFoundError(f"version.txt not found at {version_path}. This workflow requires it from the previous job.")

              with open(version_path, "r") as vfile:
                  version_name = vfile.read().strip()

              db_source = "conversation/Temp/temp.db"
              if not os.path.exists(db_source):
                  raise FileNotFoundError(f"temp.db not found at {db_source}. It should have been uploaded by the previous job.")

              os.makedirs("conversation/Ready", exist_ok=True)
              db_target = f"conversation/Ready/{version_name}.db"

              os.rename(db_source, db_target)

              with sqlite3.connect(db_target) as conn:
                  conn.execute("CREATE TABLE IF NOT EXISTS databaseversion (DBversion TEXT)")
                  conn.execute("DELETE FROM databaseversion")
                  conn.execute("INSERT INTO databaseversion (DBversion) VALUES (?)", (version_name,))
                  conn.commit()
                  conn.execute("VACUUM;")

              zip_name = f"conversation/Ready/{version_name}.zip"
              with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                  zipf.write(db_target, os.path.basename(db_target))

              log_path = "conversation/Temp/conversion_log.txt"
              with open(log_path, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 2: Finalizing with version: {version_name}\n")
                  logfile.write(f"{now} Phase 2: Finalized DB: {db_target}, Zipped to: {zip_name}\n")
                  logfile.write(f"{now} Phase 2: Conversion Complete\n")

          except Exception as e:
              os.makedirs("conversation/Temp", exist_ok=True)
              log_path_error = "conversation/Temp/conversion_log.txt"
              with open(log_path_error, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 2: Error occurred during finalization: {str(e)}\n")
                  logfile.write(traceback.format_exc())
              raise
          EOF

      - name: Commit Finalized Files and Clean Temp
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          
          git add conversation/Ready/*.db conversation/Ready/*.zip
          git add conversation/Temp/conversion_log.txt conversation/Temp/version.txt
          
          git rm conversation/Temp/temp.db || true
          
          git commit -m "Workflow complete: Merged Excel, finalized SQLite ZIP, and updated logs"
          git push
