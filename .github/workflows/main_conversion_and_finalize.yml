# .github/workflows/main_conversion_and_finalize.yml
name: Main Conversion and Finalization Workflow

on:
  push:
    paths:
      - 'conversation/Temp/file_ready_status.txt' # Triggers only when the status file is updated

jobs:
  run_full_conversion_if_ready:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Check both files and status marker for readiness
        id: check_readiness
        run: |
          STATUS_FILE="conversation/Temp/file_ready_status.txt"
          SCANNING_FILE_EXISTS=false
          PRICES_FILE_EXISTS=false
          SCANNING_STATUS_TRUE=false
          PRICES_STATUS_TRUE=false

          # 1. Check if physical files exist
          if [ -f "conversation/csv/scanning.xlsx" ]; then
            SCANNING_FILE_EXISTS=true
            echo "Scanning file physically found."
          fi
          if [ -f "conversation/csv/prices.xlsx" ]; then
            PRICES_FILE_EXISTS=true
            echo "Prices file physically found."
          fi

          # 2. Check statuses in the marker file
          if [ -f "$STATUS_FILE" ]; then
            if grep -q "scanning_ready:true" "$STATUS_FILE"; then
              SCANNING_STATUS_TRUE=true
              echo "Scanning status marked true in file_ready_status.txt."
            fi
            if grep -q "prices_ready:true" "$STATUS_FILE"; then
              PRICES_STATUS_TRUE=true
              echo "Prices status marked true in file_ready_status.txt."
            fi
          else
            echo "file_ready_status.txt not found."
          fi

          # Determine if conversion should proceed
          if $SCANNING_FILE_EXISTS && $PRICES_FILE_EXISTS && $SCANNING_STATUS_TRUE && $PRICES_STATUS_TRUE; then
            echo "All conditions met. Proceeding with full conversion."
            echo "proceed_conversion=true" >> "$GITHUB_OUTPUT"
          else
            echo "Conditions not met for full conversion. Skipping."
            echo "proceed_conversion=false" >> "$GITHUB_OUTPUT"
          fi

  # PHASE 1: Merge XLSX Files and Create Temp SQLite + Log File
  merge_and_prepare_temp_db:
    needs: run_full_conversion_if_ready # This job depends on the readiness check
    if: needs.run_full_conversion_if_ready.outputs.proceed_conversion == 'true' # Only run if readiness is true
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Required Python Packages
        run: pip install pandas openpyxl sqlite-utils

      - name: Merge XLSX Files and Create Temp SQLite + Log File (Phase 1 Logic)
        run: |
          python3 <<EOF
          import pandas as pd
          import sqlite3
          import os
          from datetime import datetime
          import traceback

          try:
              scan_file = "conversation/csv/scanning.xlsx"
              price_file = "conversation/csv/prices.xlsx"

              os.makedirs("conversation/Temp", exist_ok=True)
              log_path = "conversation/Temp/conversion_log.txt"

              with open(log_path, "w") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Workflow Started.\n")
                  logfile.write(f"{now} Processing input files.\n")

              scan_df = pd.read_excel(scan_file)
              price_df = pd.read_excel(price_file)

              price_df["OriginalPrice"] = pd.to_numeric(price_df["OriginalPrice"], errors='coerce')
              price_df = price_df[["Barcode", "OriginalPrice"]]

              merged_df = pd.merge(scan_df, price_df, on="Barcode", how="left")

              merged_df["OriginalPrice"] = merged_df["OriginalPrice"].apply(
                  lambda x: str(int(x)) if pd.notna(x) and x == int(x) else ('' if pd.isna(x) else str(x))
              )

              final_columns = ["Barcode", "Article", "Percentage", "OriginalPrice"]
              merged_df = merged_df[final_columns]

              db_name = "conversation/Temp/temp.db"
              conn = sqlite3.connect(db_name)
              merged_df.to_sql("sc", conn, if_exists="replace", index=False, dtype={'OriginalPrice': 'TEXT'})
              conn.commit()
              conn.execute("VACUUM;")
              conn.close()

              with open(log_path, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Data merged and temporary DB created.\n")

              timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
              version_name = f"DB_Version_{timestamp}"
              
              version_file_path = "conversation/Temp/version.txt"
              with open(version_file_path, "w") as vfile:
                  vfile.write(version_name + "\n")
              
              with open(log_path, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Version file updated with: '{version_name}'.\n")
                  logfile.write(f"{now} Merging Complete.\n")

          except Exception as e:
              os.makedirs("conversation/Temp", exist_ok=True)
              log_path_error = "conversation/Temp/conversion_log.txt"
              with open(log_path_error, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Error occurred in Phase 1: {str(e)}\n")
                  logfile.write(traceback.format_exc())
              raise

          EOF

      - name: Upload Temp Files for Next Job (within same workflow)
        uses: actions/upload-artifact@v4
        with:
          name: temp-conversion-artifacts
          path: |
            conversation/Temp/temp.db
            conversation/Temp/conversion_log.txt
            conversation/Temp/version.txt
          retention-days: 1 # You can adjust retention as needed

  # PHASE 2: Finalize SQLite ZIP (within same workflow)
  finalize_db_with_version:
    needs: merge_and_prepare_temp_db # This ensures Phase 2 runs only after Phase 1 succeeds
    if: needs.run_full_conversion_if_ready.outputs.proceed_conversion == 'true' # Conditional run
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Download Temp Files from Previous Job
        uses: actions/download-artifact@v4
        with:
          name: temp-conversion-artifacts
          path: conversation/Temp/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: Install Required Python Packages
        run: pip install sqlite-utils

      - name: Rename Temp DB, Zip It, Update Logs (Phase 2 Logic)
        run: |
          python3 <<EOF
          import sqlite3
          import zipfile
          import os
          from datetime import datetime
          import traceback

          try:
              version_path = "conversation/Temp/version.txt"
              db_source = "conversation/Temp/temp.db"
              log_path = "conversation/Temp/conversion_log.txt"

              if not os.path.exists(version_path):
                  raise FileNotFoundError(f"Error: version.txt not found at {version_path}. This workflow expects it to be committed by Phase 1.")
              if not os.path.exists(db_source):
                  raise FileNotFoundError(f"Error: temp.db not found at {db_source}. Phase 1 must complete successfully first.")

              with open(version_path, "r") as vfile:
                  version_name = vfile.read().strip()

              os.makedirs("conversation/Ready", exist_ok=True)
              db_target = f"conversation/Ready/{version_name}.db"

              os.rename(db_source, db_target)

              with sqlite3.connect(db_target) as conn:
                  conn.execute("CREATE TABLE IF NOT EXISTS databaseversion (DBversion TEXT)")
                  conn.execute("DELETE FROM databaseversion")
                  conn.execute("INSERT INTO databaseversion (DBversion) VALUES (?)", (version_name,))
                  conn.commit()
                  conn.execute("VACUUM;")

              zip_name = f"conversation/Ready/{version_name}.zip"
              with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                  zipf.write(db_target, os.path.basename(db_target))

              os.remove(db_target)

              with open(log_path, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 2: Version '{version_name}' applied.\n")
                  logfile.write(f"{now} Phase 2: Final ZIP created: {zip_name.split('/')[-1]}.\n")
                  logfile.write(f"{now} Phase 2: Finalization Complete.\n")

          except Exception as e:
              os.makedirs("conversation/Temp", exist_ok=True)
              log_path_error = "conversation/Temp/conversion_log.txt"
              with open(log_path_error, "a") as logfile:
                  now = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
                  logfile.write(f"{now} Phase 2: Error occurred during finalization: {str(e)}\n")
                  logfile.write(traceback.format_exc())
              raise
          EOF

      - name: Commit Finalized Files and Clean Up All Temp
        run: |
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          
          # Add the newly created finalized ZIP file
          git add conversation/Ready/*.zip
          # Add the updated log file and the version.txt
          git add conversation/Temp/conversion_log.txt conversation/Temp/version.txt
          
          # Remove temporary DB and original source files from the repository
          git rm conversation/Temp/temp.db || true
          git rm conversation/csv/scanning.xlsx || true
          git rm conversation/csv/prices.xlsx || true
          
          # Clean up the status marker file for the next run
          git rm conversation/Temp/file_ready_status.txt || true
          
          git commit -m "Full Conversion: Finalized ZIP and cleaned up all temporary/source files."
          git push